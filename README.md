首先感谢MiniMind,https://github.com/jingyaogong/minimind  
之前通过自学，学习了一些大模型的基本原理，之后又进行了几次简单实践，  
包括使用ollama部署，利用perf框架微调，mcp服务结合软件开发等。  
但随着了解的深入，感觉还是非常皮毛，于是产生了自己手写一套模型代码的想法，  
并写了一些简单的mnist,cnn,rnn. transformer的代码，但开发和训练过程全靠自己摸索，  
导致代码的效果很不理想。一度认为是自己对模型的认识产生了错误，就在这种迷茫的情况下，  
发现了 MiniMind，经过一段时间的学习和实践后， 代码效果有了不小的进步效果。  
因此将这段学习成果发布出来进行分享。因为此项目源于mini，故名little，  
也希望将来可以有小而大的成长空间。  


代码可以直接运行，主要分为3部分  
1、模型代码，LittleMind.py  
    主要包含注意力，位置编码，ffn等，由于精力原因，对minimind中的  
    一些还未熟悉的功能 进行了取舍，如moe部分海没有加进来，并对之前不甚了解，  
    但通过项目学习的地方进行了注释。  
2、训练代码，LittleTrainer.py  
    目前包含pre和sft模式，dpo部分效果不太理想，因此没有加上去，未来有效果之后  
    在进行上传。代码中去掉了wandb等外部调用，并在tools里使用方法自行创建loss曲线图。  
   使用方式为设置mode参数为，pre或者sft，data_path设置为对应的数据集文件  
    由于本人电脑缺少算力，因此进行了分割数据集为多个文件，并训练目录下所有文件的功能。  
    此功能可以中断后，继续从上次训练的文件开始继续训练。  
    同时改写了单机多卡训练的代码，直接设置参数ddp和world_size，即可进行多卡训练  
3、运行模型代码，test.py  
    test_little是手工写的推理代码，   
    test_transformer是使用框架自带的推理代码，  
    可以在使用时，对两者效果对照比较  

最后，关于之前的一些训练数据和测试效果，我保存了几张图片，放在img目录下
